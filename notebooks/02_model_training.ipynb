{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d1db08-1b7a-429f-8866-97ede53e0c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notebook: 02_model_training.py\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1. Definição da Tabela Bronze\n",
    "BRONZE_TABLE = \"dev_catalogue.staging_schema.bronze_insurance_costs\"\n",
    "SILVER_TABLE = \"dev_catalogue.staging_schema.silver_insurance_features\"\n",
    "\n",
    "# Leitura da Tabela Delta\n",
    "df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "print(f\"DataFrame lido com sucesso da tabela Delta: {BRONZE_TABLE}\")\n",
    "df_bronze.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7457fd-aea9-4b08-b161-f670785e785b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Colunas Categóricas (features que precisam de encoding)\n",
    "categorical_cols = [\"sex\", \"smoker\", \"region\"]\n",
    "\n",
    "# Cria StringIndexers para as colunas categóricas\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\")\n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "# Cria OneHotEncoders para as colunas indexadas\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\", handleInvalid=\"keep\")\n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "# Colunas Numéricas Originais (já prontas)\n",
    "numerical_cols = [\"age\", \"bmi\", \"children\"]\n",
    "\n",
    "# Colunas Finais Codificadas (as numéricas originais + as categóricas codificadas)\n",
    "final_feature_cols = numerical_cols + [col + \"_encoded\" for col in categorical_cols]\n",
    "\n",
    "# Cria o VectorAssembler para combinar todas as features em uma única coluna\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=final_feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# 3. Criação da Pipeline (SOMENTE DE TRANSFORMAÇÃO)\n",
    "pipeline = Pipeline(stages=indexers + encoders + [vector_assembler])\n",
    "\n",
    "# Aplica a Pipeline no DataFrame (ISSO CRIA 'features')\n",
    "pipeline_model = pipeline.fit(df_bronze) \n",
    "df_with_features = pipeline_model.transform(df_bronze) # DataFrame temporário com 'features'\n",
    "\n",
    "# 4. Selecionar e Salvar a Camada SILVER (APENAS COLUNAS BRUTAS + LABEL)\n",
    "# Você deve retornar ao df_bronze ou df_with_features e selecionar APENAS as colunas que a Célula 3 vai precisar para TREINAR.\n",
    "\n",
    "df_silver = df_bronze.select( # Use df_bronze, ou leia as colunas brutas do df_with_features\n",
    "    F.col(\"charges\").alias(\"label\"), \n",
    "    *numerical_cols,\n",
    "    *categorical_cols\n",
    ")\n",
    "\n",
    "# 5. Salvar a Camada SILVER\n",
    "df_silver.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "print(\"\\nDataFrame SILVER (Pré-Processado) pronto:\")\n",
    "df_silver.show(5, truncate=False)\n",
    "\n",
    "df_silver.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "print(f\"\\n✅ Camada SILVER salva com sucesso na Tabela Delta (Esquema Migrado): {SILVER_TABLE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "057440f7-aecb-4324-9b03-f2f6f0d76278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CÉLULA FINAL: Treinamento e Registro da PIPELINE COMPLETA (MLflow)\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# IMPORTAÇÕES NECESSÁRIAS:\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler # INCLUSÃO PARA DEFINIÇÃO DO PIPELINE\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "\n",
    "# Configuração da Tabela SILVER (leitura)\n",
    "SILVER_TABLE = \"dev_catalogue.staging_schema.silver_insurance_features\"\n",
    "df_silver_raw = spark.read.table(SILVER_TABLE)\n",
    "\n",
    "# --- NOVO TRECHO: Selecionar APENAS as colunas que a Pipeline precisa ---\n",
    "cols_to_use = [\"label\", \"age\", \"bmi\", \"children\", \"sex\", \"smoker\", \"region\"]\n",
    "df_silver = df_silver_raw.select(*cols_to_use)\n",
    "\n",
    "(train_df, test_df) = df_silver.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# --- DEFINIÇÕES DO PIPELINE (REINSERIDAS PARA AUTONOMIA) ---\n",
    "# -------------------------------------------------------------\n",
    "categorical_cols = [\"sex\", \"smoker\", \"region\"]\n",
    "numerical_cols = [\"age\", \"bmi\", \"children\"]\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\")\n",
    "    for col in categorical_cols\n",
    "]\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\", handleInvalid=\"keep\")\n",
    "    for col in categorical_cols\n",
    "]\n",
    "final_feature_cols = numerical_cols + [col + \"_encoded\" for col in categorical_cols]\n",
    "vector_assembler = VectorAssembler(inputCols=final_feature_cols, outputCol=\"features\")\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# --- INÍCIO DA EXECUÇÃO DO MLFLOW ---\n",
    "EXPERIMENT_PATH = \"/Users/ronaldo.rosario.ramos@gmail.com/Insurance_Cost_Prediction_Experiment\"\n",
    "MODEL_REGISTRY_NAME = \"Insurance_Cost_LR_Model\"\n",
    "ARTIFACT_PATH = \"spark_pipeline_model\" # Alterado para refletir o registro da Pipeline\n",
    "UC_VOLUME_TMP_PATH = \"/Volumes/dev_catalogue/staging_schema/raw_data_volume/mlflow_temp\"\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_PATH)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# --- AÇÃO CORRETIVA: LIMPAR CACHE (RESOLVE O OVERFLOW EXCEPTION) ---\n",
    "# -----------------------------------------------------------------\n",
    "print(\"Tentando limpar variáveis antigas para evitar Model Cache Overflow...\")\n",
    "try:\n",
    "    if 'pipeline_model_fit' in locals():\n",
    "        del pipeline_model_fit\n",
    "except:\n",
    "    pass\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "with mlflow.start_run(run_name=\"Treinamento_Pipeline_Final\") as run:\n",
    "    \n",
    "    # 1. DEFINIÇÃO E TREINAMENTO DA PIPELINE COMPLETA\n",
    "    lr = LinearRegression(featuresCol='features', labelCol='label', maxIter=10)\n",
    "    \n",
    "    # A Pipeline COMPLETA AGORA USA VARIÁVEIS DEFINIDAS NESTA CÉLULA!\n",
    "    full_pipeline = Pipeline(stages=indexers + encoders + [vector_assembler, lr])\n",
    "    \n",
    "    # O fit treina indexers e o LinearRegressor em uma só etapa\n",
    "    pipeline_model_fit = full_pipeline.fit(train_df) \n",
    "    \n",
    "    # 2. Avaliação\n",
    "    # O transform() no df_test aplica TODAS as etapas (transformação e previsão)\n",
    "    predictions_df = pipeline_model_fit.transform(test_df)\n",
    "    \n",
    "    # Cálculo de métricas...\n",
    "    evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "    rmse = evaluator.evaluate(predictions_df, {evaluator.metricName: \"rmse\"})\n",
    "    r2 = evaluator.evaluate(predictions_df, {evaluator.metricName: \"r2\"})\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "    # 3. GERAÇÃO E DEFINIÇÃO DA ASSINATURA\n",
    "    # A assinatura é inferida no DataFrame de predições (que possui 'features' e 'prediction')\n",
    "    signature = infer_signature(\n",
    "        model_input=predictions_df.select(\"features\"),\n",
    "        model_output=predictions_df.select(\"prediction\")\n",
    "    )\n",
    "\n",
    "    # 4. REGISTRO CORRIGIDO (Registrando o PipelineModel Completo)\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=pipeline_model_fit, # <-- REGISTRA A PIPELINE COMPLETA E FUNCIONAL\n",
    "        artifact_path=ARTIFACT_PATH,\n",
    "        registered_model_name=MODEL_REGISTRY_NAME,\n",
    "        dfs_tmpdir=UC_VOLUME_TMP_PATH, # <--- CORREÇÃO DE CAMINHO UC INCLUÍDA\n",
    "        signature=signature            # <--- CORREÇÃO DE ASSINATURA UC INCLUÍDA\n",
    "    )\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "print(\"-\" * 50)\n",
    "print(f\"✅ REGISTRO DA PIPELINE FINALIZADO!\")\n",
    "print(f\"   - RMSE: {rmse:.2f} | A Pipeline completa está registrada na Versão mais recente.\")\n",
    "print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_model_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
